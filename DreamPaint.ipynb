{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import ndimage\n",
    "import skvideo.io  \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import Ellipse\n",
    "from skimage.transform import rescale, resize\n",
    "from skimage import io\n",
    "import matplotlib.image as mpimg\n",
    "import imageio\n",
    "%matplotlib inline\n",
    "#plt.style.use(['dark_background'])\n",
    "#plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOPATH = '/notebooks/SkipPrediction/notebooks/brainio/video2_ds.mp4' \n",
    "EEGPATH =   '/notebooks/SkipPrediction/notebooks/brainio/eeg2_1.csv'\n",
    "BIASIMAGEPATH = '/notebooks/SkipPrediction/notebooks/brainio/illusion.jpg'\n",
    "MODELPATH = ''\n",
    "FRAMERATE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path_to_video, down_sample_rate):\n",
    "    vid = imageio.get_reader(path_to_video,  'ffmpeg')\n",
    "    h, w, c = vid.get_data(0).shape\n",
    "    dframes = int(np.ceil(vid.count_frames()/down_sample_rate))\n",
    "    pix = np.zeros((h*w*dframes,c))\n",
    "    for idx, frame in enumerate(np.arange(0, vid.count_frames(), step = down_sample_rate)):#, step = down_sample_rate)):\n",
    "        pix[h*w*idx:h*w*(idx+1),:] = vid.get_data(frame).reshape(h*w, c)\n",
    "    return pix, h, w, int(vid.count_frames()/down_sample_rate)\n",
    "    \n",
    "def load_bias(path_to_bias, h, w):\n",
    "    biasimage = io.imread(path_to_bias)\n",
    "    #biasimage = rescale(biasimage, 0.25, anti_aliasing=False)\n",
    "    biasimage = resize(biasimage, (h, w), anti_aliasing=True)\n",
    "    \n",
    "    return biasimage.reshape(h*w, 3)\n",
    "    \n",
    "def load_eeg(path_to_eeg, down_sample_rate = 10):\n",
    "    eegdata = pd.read_csv(path_to_eeg, sep = ' ', header=None)\n",
    "    eegdata = eegdata.iloc[::down_sample_rate, :]\n",
    "    return eegdata\n",
    "    \n",
    "def generate_gradient(h, w, axisrange=1):\n",
    "    return(np.meshgrid(np.linspace(-axisrange, axisrange, num=h),np.linspace(-axisrange, axisrange, num=w))[0])\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_video, path_to_eeg, path_to_bias, down_sample_rate=10, transform = None):\n",
    "        \n",
    "        self.video, h, w, num_video_frames = load_video(path_to_video, down_sample_rate)\n",
    "        self.eeg = load_eeg(path_to_eeg, down_sample_rate)\n",
    "        num_eeg_frames, _ = self.eeg.shape\n",
    "        if (num_eeg_frames == num_video_frames):\n",
    "             num_ref_frames = num_eeg_frames \n",
    "        else:\n",
    "            num_ref_frames = np.min([num_eeg_frames, num_video_frames])  \n",
    "        self.eeg = self.eeg.loc[self.eeg.index.repeat(h*w)].to_numpy() \n",
    "        xx, yy = generate_gradient(h, w), generate_gradient(w, h, axisrange=w/h).transpose()\n",
    "        zz = np.sqrt(xx**2 + yy**2)\n",
    "        gframes = np.repeat(np.concatenate((xx[:,:,np.newaxis], yy[:,:,np.newaxis], zz[:,:,np.newaxis]), axis=2)\n",
    "                            .reshape(h*w, 3), \n",
    "                            num_ref_frames, axis=0)\n",
    "        eframes = self.eeg[:num_ref_frames*h*w, :]\n",
    "        vframes = self.video[:num_ref_frames*h*w, :]\n",
    "        gframes = gframes[:num_ref_frames*h*w, :]\n",
    "        self.data = np.concatenate((gframes, vframes, eframes),axis=1)\n",
    "        self.bias = np.repeat(load_bias(path_to_bias, h, w), int(np.ceil(num_ref_frames)),axis=0)\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index,:]\n",
    "        label = self.bias[index,:]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset( VIDEOPATH, EEGPATH, BIASIMAGEPATH, transform=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape on batch size = torch.Size([64, 22])\n",
      "labels shape on batch size = torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "train_iter = iter(train_loader)\n",
    "images, labels = train_iter.next()\n",
    "\n",
    "print('images shape on batch size = {}'.format(images.size()))\n",
    "print('labels shape on batch size = {}'.format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=22, out_features=128)\n",
    "        #self.fc1_bn = nn.BatchNorm1d(num_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        #self.fc2_bn = nn.BatchNorm1d(num_features=64)\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "        #self.fc3_bn = nn.BatchNorm1d(num_features=32)\n",
    "        self.fc4 = nn.Linear(in_features = 32, out_features = 3)\n",
    "    \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.tanh(self.fc1(x)) #self.fc1_bn(self.fc1(x)))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return(x)\n",
    "    \n",
    "    def regularizer(self, reg):\n",
    "        return (self.fc1.weight.pow(2).mean() + self.fc2.weight.pow(2).mean() \n",
    "                + self.fc3.weight.pow(2).mean() + self.fc4.weight.pow(2).mean())*reg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets see the outputs for the random net tensor([[0.4714, 0.4811, 0.4305],\n",
      "        [0.4576, 0.4686, 0.4803],\n",
      "        [0.4605, 0.4699, 0.4514],\n",
      "        [0.4613, 0.4677, 0.4663],\n",
      "        [0.4621, 0.4753, 0.4582],\n",
      "        [0.4731, 0.4567, 0.4910],\n",
      "        [0.4607, 0.4703, 0.4810],\n",
      "        [0.4600, 0.4714, 0.4589],\n",
      "        [0.4572, 0.4680, 0.4964],\n",
      "        [0.4345, 0.4942, 0.4891],\n",
      "        [0.4568, 0.4627, 0.4713],\n",
      "        [0.4466, 0.4810, 0.5308],\n",
      "        [0.4592, 0.5102, 0.4314],\n",
      "        [0.4618, 0.4647, 0.4678],\n",
      "        [0.4776, 0.4551, 0.4948],\n",
      "        [0.4452, 0.4871, 0.5294],\n",
      "        [0.4661, 0.4799, 0.4392],\n",
      "        [0.4705, 0.5006, 0.4269],\n",
      "        [0.4566, 0.4706, 0.4853],\n",
      "        [0.4615, 0.4759, 0.4806],\n",
      "        [0.4576, 0.4643, 0.4730],\n",
      "        [0.4698, 0.5017, 0.4240],\n",
      "        [0.4614, 0.5102, 0.4291],\n",
      "        [0.4672, 0.5039, 0.4232],\n",
      "        [0.4697, 0.4975, 0.4287],\n",
      "        [0.4613, 0.4694, 0.4770],\n",
      "        [0.4599, 0.4761, 0.4830],\n",
      "        [0.4570, 0.4802, 0.4849],\n",
      "        [0.4470, 0.4763, 0.5183],\n",
      "        [0.4638, 0.4969, 0.4497],\n",
      "        [0.4523, 0.4764, 0.4952],\n",
      "        [0.4616, 0.4679, 0.4635],\n",
      "        [0.4671, 0.4992, 0.4251],\n",
      "        [0.4454, 0.4764, 0.5100],\n",
      "        [0.4652, 0.5015, 0.4268],\n",
      "        [0.4596, 0.5097, 0.4299],\n",
      "        [0.4656, 0.4741, 0.4426],\n",
      "        [0.4578, 0.4758, 0.4885],\n",
      "        [0.4467, 0.4972, 0.4763],\n",
      "        [0.4599, 0.4651, 0.4719],\n",
      "        [0.4624, 0.4702, 0.4556],\n",
      "        [0.4538, 0.4713, 0.5008],\n",
      "        [0.4619, 0.4724, 0.4816],\n",
      "        [0.4573, 0.4633, 0.4709],\n",
      "        [0.4622, 0.4859, 0.4471],\n",
      "        [0.4672, 0.5010, 0.4363],\n",
      "        [0.4510, 0.4796, 0.4993],\n",
      "        [0.4617, 0.5055, 0.4310],\n",
      "        [0.4755, 0.5002, 0.4226],\n",
      "        [0.4579, 0.4728, 0.4586],\n",
      "        [0.4456, 0.4767, 0.5080],\n",
      "        [0.4662, 0.4950, 0.4360],\n",
      "        [0.4702, 0.4572, 0.5271],\n",
      "        [0.4432, 0.4795, 0.5116],\n",
      "        [0.4617, 0.4712, 0.4790],\n",
      "        [0.4608, 0.5079, 0.4306],\n",
      "        [0.4585, 0.4670, 0.4640],\n",
      "        [0.4386, 0.4826, 0.5334],\n",
      "        [0.4614, 0.4669, 0.4733],\n",
      "        [0.4689, 0.4867, 0.4354],\n",
      "        [0.4574, 0.4787, 0.4860],\n",
      "        [0.4430, 0.4813, 0.5051],\n",
      "        [0.4527, 0.4687, 0.5156],\n",
      "        [0.4624, 0.5051, 0.4286]], device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net = Net().cuda()\n",
    "outputs = net(images.cuda().float()).squeeze()\n",
    "print('Lets see the outputs for the random net', outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "reg = 0\n",
    "LR = 0.00001\n",
    "#aplacel2 = LaplaceL2() \n",
    "criterion = nn.MSELoss() #nn.PoissonNLLLoss() \n",
    "optimizer = optim.Adam(net.parameters(),lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.03185\n",
      "[1,  2000] loss: 0.02684\n",
      "[1,  3000] loss: 0.02686\n",
      "[1,  4000] loss: 0.02665\n",
      "[1,  5000] loss: 0.02667\n",
      "[1,  6000] loss: 0.02672\n",
      "[1,  7000] loss: 0.02636\n",
      "[1,  8000] loss: 0.02644\n",
      "[1,  9000] loss: 0.02642\n",
      "[1, 10000] loss: 0.02630\n",
      "[1, 11000] loss: 0.02647\n",
      "[1, 12000] loss: 0.02624\n",
      "[1, 13000] loss: 0.02640\n",
      "[1, 14000] loss: 0.02624\n",
      "[1, 15000] loss: 0.02610\n",
      "[1, 16000] loss: 0.02625\n",
      "[1, 17000] loss: 0.02577\n",
      "[1, 18000] loss: 0.02569\n",
      "[1, 19000] loss: 0.02594\n",
      "[1, 20000] loss: 0.02578\n",
      "[1, 21000] loss: 0.02587\n",
      "[1, 22000] loss: 0.02586\n",
      "[1, 23000] loss: 0.02582\n",
      "[1, 24000] loss: 0.02550\n",
      "[1, 25000] loss: 0.02543\n",
      "[1, 26000] loss: 0.02573\n",
      "[1, 27000] loss: 0.02540\n",
      "[1, 28000] loss: 0.02569\n",
      "[1, 29000] loss: 0.02561\n",
      "[1, 30000] loss: 0.02553\n",
      "[1, 31000] loss: 0.02541\n",
      "[1, 32000] loss: 0.02554\n",
      "[1, 33000] loss: 0.02551\n",
      "[1, 34000] loss: 0.02548\n",
      "[1, 35000] loss: 0.02528\n",
      "[1, 36000] loss: 0.02540\n",
      "[1, 37000] loss: 0.02519\n",
      "[1, 38000] loss: 0.02511\n",
      "[1, 39000] loss: 0.02529\n",
      "[1, 40000] loss: 0.02531\n",
      "[1, 41000] loss: 0.02524\n",
      "[1, 42000] loss: 0.02528\n",
      "[1, 43000] loss: 0.02522\n",
      "[1, 44000] loss: 0.02502\n",
      "[1, 45000] loss: 0.02530\n",
      "[1, 46000] loss: 0.02504\n",
      "[1, 47000] loss: 0.02528\n",
      "[1, 48000] loss: 0.02500\n",
      "[1, 49000] loss: 0.02495\n",
      "[1, 50000] loss: 0.02503\n",
      "[1, 51000] loss: 0.02516\n",
      "[1, 52000] loss: 0.02507\n",
      "[1, 53000] loss: 0.02509\n",
      "[1, 54000] loss: 0.02526\n",
      "[1, 55000] loss: 0.02519\n",
      "[1, 56000] loss: 0.02490\n",
      "[1, 57000] loss: 0.02512\n",
      "[1, 58000] loss: 0.02514\n",
      "[1, 59000] loss: 0.02500\n",
      "[1, 60000] loss: 0.02504\n",
      "[1, 61000] loss: 0.02502\n",
      "[1, 62000] loss: 0.02511\n",
      "[1, 63000] loss: 0.02497\n",
      "[1, 64000] loss: 0.02505\n",
      "[1, 65000] loss: 0.02516\n",
      "[1, 66000] loss: 0.02500\n",
      "[1, 67000] loss: 0.02512\n",
      "[1, 68000] loss: 0.02504\n",
      "[1, 69000] loss: 0.02493\n",
      "[1, 70000] loss: 0.02514\n",
      "[1, 71000] loss: 0.02505\n",
      "[1, 72000] loss: 0.02490\n",
      "[1, 73000] loss: 0.02523\n",
      "[1, 74000] loss: 0.02492\n",
      "[1, 75000] loss: 0.02490\n",
      "[1, 76000] loss: 0.02492\n",
      "[1, 77000] loss: 0.02498\n",
      "[1, 78000] loss: 0.02498\n",
      "[1, 79000] loss: 0.02489\n",
      "[1, 80000] loss: 0.02494\n",
      "[1, 81000] loss: 0.02498\n",
      "[1, 82000] loss: 0.02483\n",
      "[1, 83000] loss: 0.02484\n",
      "[1, 84000] loss: 0.02485\n",
      "[1, 85000] loss: 0.02496\n",
      "[1, 86000] loss: 0.02485\n",
      "[1, 87000] loss: 0.02482\n",
      "[1, 88000] loss: 0.02487\n",
      "[1, 89000] loss: 0.02481\n",
      "[1, 90000] loss: 0.02498\n",
      "[1, 91000] loss: 0.02492\n",
      "[1, 92000] loss: 0.02476\n",
      "[1, 93000] loss: 0.02491\n",
      "[1, 94000] loss: 0.02484\n",
      "[1, 95000] loss: 0.02486\n",
      "[1, 96000] loss: 0.02481\n",
      "[1, 97000] loss: 0.02475\n",
      "[1, 98000] loss: 0.02484\n",
      "[1, 99000] loss: 0.02478\n",
      "[1, 100000] loss: 0.02471\n",
      "[1, 101000] loss: 0.02471\n",
      "[1, 102000] loss: 0.02484\n",
      "[1, 103000] loss: 0.02483\n",
      "[1, 104000] loss: 0.02479\n",
      "[1, 105000] loss: 0.02475\n",
      "[1, 106000] loss: 0.02492\n",
      "[1, 107000] loss: 0.02501\n",
      "[1, 108000] loss: 0.02487\n",
      "[1, 109000] loss: 0.02467\n",
      "[1, 110000] loss: 0.02477\n",
      "[1, 111000] loss: 0.02487\n",
      "[1, 112000] loss: 0.02478\n",
      "[1, 113000] loss: 0.02481\n",
      "[1, 114000] loss: 0.02492\n",
      "[1, 115000] loss: 0.02468\n",
      "[1, 116000] loss: 0.02492\n",
      "[1, 117000] loss: 0.02497\n",
      "[1, 118000] loss: 0.02480\n",
      "[1, 119000] loss: 0.02485\n",
      "[1, 120000] loss: 0.02487\n",
      "[1, 121000] loss: 0.02474\n",
      "[1, 122000] loss: 0.02494\n",
      "[1, 123000] loss: 0.02463\n",
      "[1, 124000] loss: 0.02457\n",
      "[1, 125000] loss: 0.02477\n",
      "[1, 126000] loss: 0.02481\n",
      "[1, 127000] loss: 0.02482\n",
      "[1, 128000] loss: 0.02480\n",
      "[1, 129000] loss: 0.02457\n",
      "[1, 130000] loss: 0.02478\n",
      "[1, 131000] loss: 0.02479\n",
      "[1, 132000] loss: 0.02468\n",
      "[1, 133000] loss: 0.02481\n",
      "[1, 134000] loss: 0.02473\n",
      "[1, 135000] loss: 0.02481\n",
      "[1, 136000] loss: 0.02472\n",
      "[1, 137000] loss: 0.02469\n",
      "[1, 138000] loss: 0.02478\n",
      "[1, 139000] loss: 0.02467\n",
      "[1, 140000] loss: 0.02464\n",
      "[1, 141000] loss: 0.02479\n",
      "[1, 142000] loss: 0.02489\n",
      "[1, 143000] loss: 0.02461\n",
      "[1, 144000] loss: 0.02480\n",
      "[1, 145000] loss: 0.02461\n",
      "[1, 146000] loss: 0.02473\n",
      "[1, 147000] loss: 0.02465\n",
      "[1, 148000] loss: 0.02474\n",
      "[1, 149000] loss: 0.02468\n",
      "[1, 150000] loss: 0.02462\n",
      "[1, 151000] loss: 0.02444\n",
      "[1, 152000] loss: 0.02468\n",
      "[1, 153000] loss: 0.02479\n",
      "[1, 154000] loss: 0.02467\n",
      "[1, 155000] loss: 0.02471\n",
      "[1, 156000] loss: 0.02475\n",
      "[1, 157000] loss: 0.02456\n",
      "[1, 158000] loss: 0.02468\n",
      "[1, 159000] loss: 0.02467\n",
      "[1, 160000] loss: 0.02468\n",
      "[1, 161000] loss: 0.02458\n",
      "[1, 162000] loss: 0.02448\n",
      "[1, 163000] loss: 0.02477\n",
      "[1, 164000] loss: 0.02456\n",
      "[1, 165000] loss: 0.02469\n",
      "[1, 166000] loss: 0.02459\n",
      "[1, 167000] loss: 0.02451\n",
      "[1, 168000] loss: 0.02467\n",
      "[1, 169000] loss: 0.02481\n",
      "[1, 170000] loss: 0.02456\n",
      "[1, 171000] loss: 0.02460\n",
      "[1, 172000] loss: 0.02477\n",
      "[1, 173000] loss: 0.02456\n",
      "[1, 174000] loss: 0.02468\n",
      "[1, 175000] loss: 0.02483\n",
      "[1, 176000] loss: 0.02463\n",
      "[1, 177000] loss: 0.02450\n",
      "[1, 178000] loss: 0.02459\n",
      "[1, 179000] loss: 0.02464\n",
      "[1, 180000] loss: 0.02461\n",
      "[1, 181000] loss: 0.02471\n",
      "[1, 182000] loss: 0.02448\n",
      "[1, 183000] loss: 0.02457\n",
      "[1, 184000] loss: 0.02456\n",
      "[1, 185000] loss: 0.02472\n",
      "[1, 186000] loss: 0.02463\n",
      "[1, 187000] loss: 0.02468\n",
      "[1, 188000] loss: 0.02453\n",
      "[1, 189000] loss: 0.02452\n",
      "[1, 190000] loss: 0.02456\n",
      "[1, 191000] loss: 0.02458\n",
      "[1, 192000] loss: 0.02465\n",
      "[1, 193000] loss: 0.02470\n",
      "[1, 194000] loss: 0.02455\n",
      "[1, 195000] loss: 0.02465\n",
      "[1, 196000] loss: 0.02466\n",
      "[1, 197000] loss: 0.02460\n",
      "[1, 198000] loss: 0.02454\n",
      "[1, 199000] loss: 0.02455\n",
      "[1, 200000] loss: 0.02448\n",
      "[1, 201000] loss: 0.02450\n",
      "[1, 202000] loss: 0.02464\n",
      "[1, 203000] loss: 0.02475\n",
      "[1, 204000] loss: 0.02462\n",
      "[1, 205000] loss: 0.02471\n",
      "[1, 206000] loss: 0.02451\n",
      "[1, 207000] loss: 0.02451\n",
      "[1, 208000] loss: 0.02470\n",
      "[1, 209000] loss: 0.02463\n",
      "[1, 210000] loss: 0.02460\n",
      "[1, 211000] loss: 0.02460\n",
      "[1, 212000] loss: 0.02466\n",
      "[1, 213000] loss: 0.02455\n",
      "[1, 214000] loss: 0.02460\n",
      "[1, 215000] loss: 0.02474\n",
      "[1, 216000] loss: 0.02448\n",
      "[1, 217000] loss: 0.02461\n",
      "[1, 218000] loss: 0.02445\n",
      "[1, 219000] loss: 0.02439\n",
      "[1, 220000] loss: 0.02451\n",
      "[1, 221000] loss: 0.02447\n",
      "[1, 222000] loss: 0.02473\n",
      "[1, 223000] loss: 0.02470\n",
      "[1, 224000] loss: 0.02437\n",
      "[1, 225000] loss: 0.02455\n",
      "[1, 226000] loss: 0.02447\n",
      "[1, 227000] loss: 0.02459\n",
      "[1, 228000] loss: 0.02461\n",
      "[1, 229000] loss: 0.02465\n",
      "[1, 230000] loss: 0.02441\n",
      "[1, 231000] loss: 0.02476\n",
      "[1, 232000] loss: 0.02464\n",
      "[1, 233000] loss: 0.02443\n",
      "[1, 234000] loss: 0.02460\n",
      "[1, 235000] loss: 0.02458\n",
      "[1, 236000] loss: 0.02468\n",
      "[1, 237000] loss: 0.02445\n",
      "[1, 238000] loss: 0.02460\n",
      "[1, 239000] loss: 0.02449\n",
      "[1, 240000] loss: 0.02450\n",
      "[1, 241000] loss: 0.02452\n",
      "[1, 242000] loss: 0.02447\n",
      "[1, 243000] loss: 0.02472\n",
      "[1, 244000] loss: 0.02468\n",
      "[1, 245000] loss: 0.02445\n",
      "[1, 246000] loss: 0.02459\n",
      "[1, 247000] loss: 0.02482\n",
      "[1, 248000] loss: 0.02428\n",
      "[1, 249000] loss: 0.02469\n",
      "[1, 250000] loss: 0.02437\n",
      "[1, 251000] loss: 0.02437\n",
      "[1, 252000] loss: 0.02449\n",
      "[1, 253000] loss: 0.02464\n",
      "[1, 254000] loss: 0.02444\n",
      "[1, 255000] loss: 0.02458\n",
      "[1, 256000] loss: 0.02437\n",
      "[1, 257000] loss: 0.02443\n",
      "[1, 258000] loss: 0.02438\n",
      "[1, 259000] loss: 0.02454\n",
      "[1, 260000] loss: 0.02465\n",
      "[1, 261000] loss: 0.02445\n",
      "[1, 262000] loss: 0.02450\n",
      "[1, 263000] loss: 0.02448\n",
      "[1, 264000] loss: 0.02450\n",
      "[1, 265000] loss: 0.02445\n",
      "[1, 266000] loss: 0.02454\n",
      "[1, 267000] loss: 0.02445\n",
      "[1, 268000] loss: 0.02455\n",
      "[1, 269000] loss: 0.02474\n",
      "[1, 270000] loss: 0.02452\n",
      "[1, 271000] loss: 0.02451\n",
      "[1, 272000] loss: 0.02448\n",
      "[1, 273000] loss: 0.02451\n",
      "[1, 274000] loss: 0.02467\n",
      "[1, 275000] loss: 0.02456\n",
      "[1, 276000] loss: 0.02454\n",
      "[1, 277000] loss: 0.02447\n",
      "[1, 278000] loss: 0.02462\n",
      "[1, 279000] loss: 0.02454\n",
      "[1, 280000] loss: 0.02462\n",
      "[1, 281000] loss: 0.02448\n",
      "[1, 282000] loss: 0.02449\n",
      "[1, 283000] loss: 0.02463\n",
      "[1, 284000] loss: 0.02460\n",
      "[1, 285000] loss: 0.02453\n",
      "[1, 286000] loss: 0.02449\n",
      "[1, 287000] loss: 0.02431\n",
      "[1, 288000] loss: 0.02455\n",
      "[1, 289000] loss: 0.02438\n",
      "[1, 290000] loss: 0.02463\n",
      "[1, 291000] loss: 0.02454\n",
      "[1, 292000] loss: 0.02468\n",
      "[1, 293000] loss: 0.02446\n",
      "[1, 294000] loss: 0.02456\n",
      "[1, 295000] loss: 0.02455\n",
      "[1, 296000] loss: 0.02464\n",
      "[1, 297000] loss: 0.02448\n",
      "[1, 298000] loss: 0.02455\n",
      "[1, 299000] loss: 0.02449\n",
      "[1, 300000] loss: 0.02446\n",
      "[1, 301000] loss: 0.02444\n",
      "[1, 302000] loss: 0.02438\n",
      "[1, 303000] loss: 0.02464\n",
      "[1, 304000] loss: 0.02439\n",
      "[1, 305000] loss: 0.02418\n",
      "[1, 306000] loss: 0.02449\n",
      "[1, 307000] loss: 0.02458\n",
      "[1, 308000] loss: 0.02459\n",
      "[1, 309000] loss: 0.02455\n",
      "[1, 310000] loss: 0.02447\n",
      "[1, 311000] loss: 0.02444\n",
      "[1, 312000] loss: 0.02442\n",
      "[1, 313000] loss: 0.02449\n",
      "[1, 314000] loss: 0.02456\n",
      "[1, 315000] loss: 0.02437\n",
      "[1, 316000] loss: 0.02442\n",
      "[1, 317000] loss: 0.02448\n",
      "[1, 318000] loss: 0.02449\n",
      "[1, 319000] loss: 0.02451\n",
      "[1, 320000] loss: 0.02452\n",
      "[1, 321000] loss: 0.02457\n",
      "[1, 322000] loss: 0.02444\n",
      "[1, 323000] loss: 0.02432\n",
      "[1, 324000] loss: 0.02454\n",
      "[1, 325000] loss: 0.02434\n",
      "[1, 326000] loss: 0.02449\n",
      "[1, 327000] loss: 0.02447\n",
      "[1, 328000] loss: 0.02449\n",
      "[1, 329000] loss: 0.02424\n",
      "[1, 330000] loss: 0.02444\n",
      "[1, 331000] loss: 0.02455\n",
      "[1, 332000] loss: 0.02434\n",
      "[1, 333000] loss: 0.02447\n",
      "[1, 334000] loss: 0.02428\n",
      "[1, 335000] loss: 0.02460\n",
      "[1, 336000] loss: 0.02457\n",
      "[1, 337000] loss: 0.02446\n",
      "[1, 338000] loss: 0.02441\n",
      "[1, 339000] loss: 0.02448\n",
      "[1, 340000] loss: 0.02459\n",
      "[1, 341000] loss: 0.02431\n",
      "[1, 342000] loss: 0.02433\n",
      "[1, 343000] loss: 0.02436\n",
      "[1, 344000] loss: 0.02421\n",
      "[1, 345000] loss: 0.02424\n",
      "[1, 346000] loss: 0.02415\n",
      "[1, 347000] loss: 0.02454\n",
      "[1, 348000] loss: 0.02457\n",
      "[1, 349000] loss: 0.02448\n",
      "[1, 350000] loss: 0.02445\n",
      "[1, 351000] loss: 0.02454\n",
      "[1, 352000] loss: 0.02427\n",
      "[1, 353000] loss: 0.02453\n",
      "[1, 354000] loss: 0.02441\n",
      "[1, 355000] loss: 0.02440\n",
      "[1, 356000] loss: 0.02435\n",
      "[1, 357000] loss: 0.02443\n",
      "[1, 358000] loss: 0.02431\n",
      "[1, 359000] loss: 0.02424\n",
      "[1, 360000] loss: 0.02437\n",
      "[1, 361000] loss: 0.02456\n",
      "[1, 362000] loss: 0.02433\n",
      "[1, 363000] loss: 0.02444\n",
      "[1, 364000] loss: 0.02444\n",
      "[1, 365000] loss: 0.02466\n",
      "[1, 366000] loss: 0.02450\n",
      "[1, 367000] loss: 0.02449\n",
      "[1, 368000] loss: 0.02444\n",
      "[1, 369000] loss: 0.02456\n",
      "[1, 370000] loss: 0.02435\n",
      "[1, 371000] loss: 0.02439\n",
      "[1, 372000] loss: 0.02445\n",
      "[1, 373000] loss: 0.02424\n",
      "[1, 374000] loss: 0.02444\n",
      "[1, 375000] loss: 0.02457\n",
      "[1, 376000] loss: 0.02452\n",
      "[1, 377000] loss: 0.02438\n",
      "[1, 378000] loss: 0.02443\n",
      "[1, 379000] loss: 0.02446\n",
      "[1, 380000] loss: 0.02457\n",
      "[1, 381000] loss: 0.02433\n",
      "[1, 382000] loss: 0.02444\n",
      "[1, 383000] loss: 0.02436\n",
      "[1, 384000] loss: 0.02449\n",
      "[1, 385000] loss: 0.02426\n",
      "[1, 386000] loss: 0.02434\n",
      "[1, 387000] loss: 0.02451\n",
      "[1, 388000] loss: 0.02446\n",
      "[1, 389000] loss: 0.02425\n",
      "[1, 390000] loss: 0.02448\n",
      "[1, 391000] loss: 0.02438\n",
      "[1, 392000] loss: 0.02419\n",
      "[1, 393000] loss: 0.02442\n",
      "[1, 394000] loss: 0.02439\n",
      "[1, 395000] loss: 0.02430\n",
      "[1, 396000] loss: 0.02445\n",
      "[1, 397000] loss: 0.02435\n",
      "[1, 398000] loss: 0.02425\n",
      "[1, 399000] loss: 0.02447\n",
      "[1, 400000] loss: 0.02457\n",
      "[1, 401000] loss: 0.02437\n",
      "[1, 402000] loss: 0.02448\n",
      "[1, 403000] loss: 0.02450\n",
      "[1, 404000] loss: 0.02434\n",
      "[1, 405000] loss: 0.02446\n",
      "[1, 406000] loss: 0.02444\n",
      "[1, 407000] loss: 0.02423\n",
      "[1, 408000] loss: 0.02449\n",
      "[1, 409000] loss: 0.02430\n",
      "[1, 410000] loss: 0.02440\n",
      "[1, 411000] loss: 0.02422\n",
      "[1, 412000] loss: 0.02426\n",
      "[1, 413000] loss: 0.02423\n",
      "[1, 414000] loss: 0.02435\n",
      "[1, 415000] loss: 0.02434\n",
      "[1, 416000] loss: 0.02423\n",
      "[1, 417000] loss: 0.02449\n",
      "[1, 418000] loss: 0.02425\n",
      "[1, 419000] loss: 0.02430\n",
      "[1, 420000] loss: 0.02434\n",
      "[1, 421000] loss: 0.02423\n",
      "[1, 422000] loss: 0.02433\n",
      "[1, 423000] loss: 0.02418\n",
      "[1, 424000] loss: 0.02445\n",
      "[1, 425000] loss: 0.02437\n",
      "[1, 426000] loss: 0.02443\n",
      "[1, 427000] loss: 0.02429\n",
      "[1, 428000] loss: 0.02458\n",
      "[1, 429000] loss: 0.02441\n",
      "[1, 430000] loss: 0.02437\n",
      "[1, 431000] loss: 0.02439\n",
      "[1, 432000] loss: 0.02421\n",
      "[1, 433000] loss: 0.02428\n",
      "[1, 434000] loss: 0.02434\n",
      "[1, 435000] loss: 0.02460\n",
      "[1, 436000] loss: 0.02424\n",
      "[1, 437000] loss: 0.02433\n",
      "[1, 438000] loss: 0.02432\n",
      "[1, 439000] loss: 0.02444\n",
      "[1, 440000] loss: 0.02422\n",
      "[1, 441000] loss: 0.02423\n",
      "[1, 442000] loss: 0.02439\n",
      "[1, 443000] loss: 0.02450\n",
      "[1, 444000] loss: 0.02426\n",
      "[1, 445000] loss: 0.02449\n",
      "[1, 446000] loss: 0.02448\n",
      "[1, 447000] loss: 0.02441\n",
      "[1, 448000] loss: 0.02440\n",
      "[1, 449000] loss: 0.02454\n",
      "[1, 450000] loss: 0.02432\n",
      "[1, 451000] loss: 0.02441\n",
      "[1, 452000] loss: 0.02441\n",
      "[1, 453000] loss: 0.02432\n",
      "[1, 454000] loss: 0.02440\n",
      "[1, 455000] loss: 0.02432\n",
      "[1, 456000] loss: 0.02431\n",
      "[1, 457000] loss: 0.02461\n",
      "[1, 458000] loss: 0.02429\n",
      "[1, 459000] loss: 0.02448\n",
      "[1, 460000] loss: 0.02447\n",
      "[1, 461000] loss: 0.02429\n",
      "[1, 462000] loss: 0.02433\n",
      "[1, 463000] loss: 0.02438\n",
      "[1, 464000] loss: 0.02443\n",
      "[1, 465000] loss: 0.02445\n",
      "[1, 466000] loss: 0.02439\n",
      "[1, 467000] loss: 0.02428\n",
      "[1, 468000] loss: 0.02438\n",
      "[1, 469000] loss: 0.02434\n",
      "[1, 470000] loss: 0.02435\n",
      "[1, 471000] loss: 0.02447\n",
      "[1, 472000] loss: 0.02441\n",
      "[1, 473000] loss: 0.02452\n",
      "[1, 474000] loss: 0.02436\n",
      "[1, 475000] loss: 0.02430\n",
      "[1, 476000] loss: 0.02431\n",
      "[1, 477000] loss: 0.02430\n",
      "[1, 478000] loss: 0.02428\n",
      "[1, 479000] loss: 0.02416\n",
      "[1, 480000] loss: 0.02424\n",
      "[1, 481000] loss: 0.02442\n",
      "[1, 482000] loss: 0.02425\n",
      "[1, 483000] loss: 0.02446\n",
      "[1, 484000] loss: 0.02429\n",
      "[1, 485000] loss: 0.02430\n",
      "[1, 486000] loss: 0.02433\n",
      "[1, 487000] loss: 0.02435\n",
      "[1, 488000] loss: 0.02424\n",
      "[1, 489000] loss: 0.02441\n",
      "[1, 490000] loss: 0.02419\n",
      "[1, 491000] loss: 0.02430\n",
      "[1, 492000] loss: 0.02412\n",
      "[1, 493000] loss: 0.02441\n",
      "[1, 494000] loss: 0.02427\n",
      "[1, 495000] loss: 0.02439\n",
      "[1, 496000] loss: 0.02424\n",
      "[1, 497000] loss: 0.02435\n",
      "[1, 498000] loss: 0.02419\n",
      "[1, 499000] loss: 0.02413\n",
      "[1, 500000] loss: 0.02452\n",
      "[1, 501000] loss: 0.02405\n",
      "[1, 502000] loss: 0.02442\n",
      "[1, 503000] loss: 0.02433\n",
      "[1, 504000] loss: 0.02439\n",
      "[1, 505000] loss: 0.02435\n",
      "[1, 506000] loss: 0.02438\n",
      "[1, 507000] loss: 0.02426\n",
      "[1, 508000] loss: 0.02414\n",
      "[1, 509000] loss: 0.02440\n",
      "[1, 510000] loss: 0.02424\n",
      "[1, 511000] loss: 0.02433\n",
      "[1, 512000] loss: 0.02450\n",
      "[1, 513000] loss: 0.02438\n",
      "[1, 514000] loss: 0.02436\n",
      "[1, 515000] loss: 0.02412\n",
      "[1, 516000] loss: 0.02431\n",
      "[1, 517000] loss: 0.02429\n",
      "[1, 518000] loss: 0.02441\n",
      "[1, 519000] loss: 0.02412\n",
      "[1, 520000] loss: 0.02426\n",
      "[1, 521000] loss: 0.02440\n",
      "[1, 522000] loss: 0.02446\n",
      "[1, 523000] loss: 0.02449\n",
      "[1, 524000] loss: 0.02411\n",
      "[1, 525000] loss: 0.02423\n",
      "[1, 526000] loss: 0.02420\n",
      "[1, 527000] loss: 0.02422\n",
      "[1, 528000] loss: 0.02432\n",
      "[1, 529000] loss: 0.02434\n",
      "[1, 530000] loss: 0.02433\n",
      "[1, 531000] loss: 0.02443\n",
      "[1, 532000] loss: 0.02427\n",
      "[1, 533000] loss: 0.02429\n",
      "[1, 534000] loss: 0.02426\n",
      "[1, 535000] loss: 0.02416\n",
      "[1, 536000] loss: 0.02414\n",
      "[1, 537000] loss: 0.02431\n",
      "[1, 538000] loss: 0.02438\n",
      "[1, 539000] loss: 0.02434\n",
      "[1, 540000] loss: 0.02411\n",
      "[1, 541000] loss: 0.02432\n",
      "[1, 542000] loss: 0.02424\n",
      "[1, 543000] loss: 0.02424\n",
      "[1, 544000] loss: 0.02433\n",
      "[1, 545000] loss: 0.02433\n",
      "[1, 546000] loss: 0.02421\n",
      "[1, 547000] loss: 0.02433\n",
      "[1, 548000] loss: 0.02433\n",
      "[1, 549000] loss: 0.02449\n",
      "[1, 550000] loss: 0.02421\n",
      "[1, 551000] loss: 0.02436\n",
      "[1, 552000] loss: 0.02423\n",
      "[1, 553000] loss: 0.02437\n",
      "[1, 554000] loss: 0.02419\n",
      "[1, 555000] loss: 0.02430\n",
      "[1, 556000] loss: 0.02441\n",
      "[1, 557000] loss: 0.02420\n",
      "[1, 558000] loss: 0.02435\n",
      "[1, 559000] loss: 0.02428\n",
      "[1, 560000] loss: 0.02408\n",
      "[1, 561000] loss: 0.02427\n",
      "[1, 562000] loss: 0.02437\n",
      "[1, 563000] loss: 0.02446\n",
      "[1, 564000] loss: 0.02424\n",
      "[1, 565000] loss: 0.02424\n",
      "[1, 566000] loss: 0.02419\n",
      "[1, 567000] loss: 0.02454\n",
      "[1, 568000] loss: 0.02418\n",
      "[1, 569000] loss: 0.02440\n",
      "[1, 570000] loss: 0.02436\n",
      "[1, 571000] loss: 0.02431\n",
      "[1, 572000] loss: 0.02425\n",
      "[1, 573000] loss: 0.02452\n",
      "[1, 574000] loss: 0.02429\n",
      "[1, 575000] loss: 0.02427\n",
      "[1, 576000] loss: 0.02436\n",
      "[1, 577000] loss: 0.02421\n",
      "[1, 578000] loss: 0.02431\n",
      "[1, 579000] loss: 0.02445\n",
      "[1, 580000] loss: 0.02424\n",
      "[1, 581000] loss: 0.02425\n",
      "[1, 582000] loss: 0.02411\n",
      "[1, 583000] loss: 0.02418\n",
      "[1, 584000] loss: 0.02412\n",
      "[1, 585000] loss: 0.02434\n",
      "[1, 586000] loss: 0.02415\n",
      "[1, 587000] loss: 0.02420\n",
      "[1, 588000] loss: 0.02421\n",
      "[1, 589000] loss: 0.02424\n",
      "[1, 590000] loss: 0.02422\n",
      "[1, 591000] loss: 0.02420\n",
      "[1, 592000] loss: 0.02438\n",
      "[1, 593000] loss: 0.02432\n",
      "[1, 594000] loss: 0.02395\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 1\n",
    "\n",
    "for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the input\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.cuda().float()).squeeze()\n",
    "        labels = labels.float()\n",
    "        #loss = criterion(outputs, labels)  #labels.cuda())\n",
    "        loss = criterion(outputs, labels.cuda()) + net.regularizer(reg)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "x_layer = 0\n",
    "y_layer = 1\n",
    "r_layer = 2\n",
    "num_eeg_layers = 8\n",
    "eeg_layers = range(r_layer, r_layer+num_eeg_layers)\n",
    "\n",
    "# ALLOCATION\n",
    "grouped = []\n",
    "\n",
    "for idx, layer in enumerate(net.named_parameters()):\n",
    "    if idx==0:\n",
    "        weights = layer[1].detach().cpu().numpy().transpose()\n",
    "        grouped.append({'weights': weights[r_layer+4:,:], \n",
    "                       'bias':  np.array(0),\n",
    "                       'activation': 'tanh'})\n",
    "        grouped.append({'weights': weights[x_layer, :][np.newaxis,:], \n",
    "                       'bias': np.array(0),\n",
    "                       'activation': 'tanh'})\n",
    "        grouped.append({'weights': weights[y_layer, :][np.newaxis,:], \n",
    "                       'bias':  np.array(0),\n",
    "                       'activation': 'tanh'})\n",
    "        grouped.append({'weights': weights[r_layer,:][np.newaxis,:], \n",
    "                       'bias':  np.array(0),\n",
    "                       'activation': 'tanh'})\n",
    "        for i in np.arange(3):\n",
    "            grouped.append({'weights': weights[r_layer+i, :][np.newaxis,:], \n",
    "                           'bias':  np.array(0),\n",
    "                           'activation': 'tanh'})  \n",
    "        grouped.append({'weights': np.zeros((128))[np.newaxis,:], \n",
    "                       'bias':  np.array(0),\n",
    "                       'activation': 'tanh'})\n",
    "    elif idx==1:\n",
    "        continue\n",
    "    elif idx%2==0:\n",
    "        weights = layer[1].detach().cpu().numpy().transpose()\n",
    "    else:\n",
    "        # extract biases\n",
    "        bias = layer[1].detach().cpu().numpy().transpose()\n",
    "        grouped.append({'weights': weights, \n",
    "                       'bias': bias,\n",
    "                       'activation': 'tanh'})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.13419105,  0.10357777,  0.09121027, ..., -0.11948179,\n",
       "         -0.14664254, -0.09729779],\n",
       "        [ 0.02213232,  0.03351563, -0.04989863, ...,  0.07303265,\n",
       "          0.21928878,  0.18077382],\n",
       "        [ 0.01961983,  0.0156246 ,  0.07296897, ..., -0.05238726,\n",
       "          0.11031671, -0.23523319],\n",
       "        ...,\n",
       "        [ 0.02425811,  0.01026146, -0.05588618, ..., -0.00031853,\n",
       "          0.09709542, -0.02615653],\n",
       "        [-0.03386963, -0.1708096 , -0.0158639 , ...,  0.11939831,\n",
       "          0.2771896 ,  0.02156863],\n",
       "        [ 0.11026907,  0.06282111, -0.18265785, ..., -0.05967565,\n",
       "          0.07297625, -0.11889093]], dtype=float32),\n",
       " array([[-9.32326391e-02,  7.17796683e-02,  1.87051475e-01,\n",
       "          2.58743942e-01, -8.22711363e-02, -4.07451317e-02,\n",
       "         -2.14901239e-01, -2.18394637e-01, -9.62023586e-02,\n",
       "         -1.99569330e-01,  6.86448812e-02,  5.59678338e-02,\n",
       "         -2.52464954e-02,  8.37474689e-02, -3.95889208e-02,\n",
       "         -5.86593710e-02,  1.78345591e-01, -1.02875866e-01,\n",
       "         -2.03005821e-01,  3.91162075e-02,  3.77664983e-01,\n",
       "          4.20528138e-03,  1.14452988e-01,  3.39282602e-01,\n",
       "         -1.82120785e-01, -7.00916201e-02,  8.62494297e-03,\n",
       "         -1.03035178e-02, -6.95535541e-02,  1.22539103e-01,\n",
       "         -9.83916298e-02,  3.14337373e-01, -1.24205992e-01,\n",
       "          1.76888391e-01, -6.59521073e-02, -1.30560964e-01,\n",
       "         -2.16886595e-01, -2.95107424e-01, -4.27225605e-02,\n",
       "          4.78041098e-02, -1.27462506e-01, -4.29843180e-02,\n",
       "         -2.26032212e-01, -9.29372683e-02, -2.55012482e-01,\n",
       "          7.92156979e-02, -5.98099381e-02, -9.87446681e-02,\n",
       "         -1.09053746e-01,  6.27144575e-02,  2.78939027e-02,\n",
       "         -2.73210257e-02,  2.41333283e-02,  3.32074501e-02,\n",
       "          2.97123045e-01, -7.86978081e-02, -1.00853153e-01,\n",
       "         -3.12065496e-03, -9.47727039e-02, -1.19249530e-01,\n",
       "          4.18228209e-01,  1.73045337e-01,  1.18750960e-01,\n",
       "         -1.35153458e-01,  2.23864410e-02,  4.22675461e-01,\n",
       "         -1.44626632e-01, -8.92700255e-02, -5.71569502e-02,\n",
       "         -1.40429452e-01, -1.74532652e-01,  6.41172454e-02,\n",
       "          6.83304816e-02,  2.89495111e-01, -1.55821919e-01,\n",
       "         -1.14037715e-01,  5.62328920e-02,  8.70513823e-03,\n",
       "          1.55004695e-01, -1.56758845e-01,  5.27802631e-02,\n",
       "         -5.70752472e-02,  1.97645709e-01,  1.04155771e-01,\n",
       "          1.76858380e-02,  2.66142040e-02, -2.36904323e-02,\n",
       "          1.05500892e-01,  9.86465216e-02,  2.93634832e-01,\n",
       "         -2.10321043e-02, -1.02407686e-01,  8.91958177e-02,\n",
       "          8.72184895e-03, -1.43265367e-01,  1.02225766e-01,\n",
       "         -2.43311457e-04, -1.03617847e-01,  1.42175198e-01,\n",
       "          1.22305885e-01, -5.36351874e-02, -9.59150307e-03,\n",
       "          7.46229067e-02,  1.20901063e-01,  1.25324562e-01,\n",
       "         -1.40750259e-01,  4.25714016e-01,  4.70650867e-02,\n",
       "          4.91904803e-02,  1.12050483e-02, -7.52198622e-02,\n",
       "          2.12821215e-01,  3.83502722e-01, -7.07450882e-02,\n",
       "          1.20435636e-02, -1.17431534e-02, -5.33789061e-02,\n",
       "          9.41749439e-02,  1.63966000e-01, -1.61269650e-01,\n",
       "          2.06706356e-02,  1.06027640e-01,  2.38565151e-02,\n",
       "          8.65928158e-02, -1.98749363e-01, -1.21606722e-01,\n",
       "         -2.74264365e-02,  3.34094814e-03]], dtype=float32),\n",
       " array([[ 2.65526503e-01, -2.37182111e-01, -1.11327674e-02,\n",
       "         -7.74799511e-02, -1.76712334e-01, -1.40952185e-01,\n",
       "         -2.09273592e-01, -4.46282215e-02, -2.01170325e-01,\n",
       "         -1.85537726e-01,  1.55137509e-01, -2.01523781e-01,\n",
       "          2.10053220e-01,  2.21374050e-01, -5.55147156e-02,\n",
       "          7.43702427e-02, -2.17425764e-01, -1.90971827e-03,\n",
       "         -1.88816205e-01, -8.60526189e-02,  1.26156285e-01,\n",
       "         -6.06888458e-02,  1.30997613e-01,  1.45493045e-01,\n",
       "          1.67654917e-01,  7.79022574e-02, -1.19601808e-01,\n",
       "         -1.67901604e-03,  2.14010835e-01,  3.72348689e-02,\n",
       "          1.72492519e-01,  7.19024763e-02,  1.06813796e-01,\n",
       "          1.21051162e-01, -6.42719343e-02,  7.98891410e-02,\n",
       "          1.91782936e-01,  5.79264089e-02,  9.10297707e-02,\n",
       "         -1.35056928e-01, -1.83826350e-02,  4.54075336e-01,\n",
       "          2.75504589e-02, -2.09787473e-01,  1.98301852e-01,\n",
       "         -2.73274571e-01,  1.47905990e-01,  1.06287971e-01,\n",
       "          3.39276761e-01,  7.38286525e-02,  3.02158594e-01,\n",
       "          2.44771913e-01,  2.07869157e-01, -4.87634316e-02,\n",
       "         -1.98336288e-01,  1.84801549e-01,  3.80032867e-01,\n",
       "          2.25866660e-01, -1.78138211e-01, -4.51355986e-02,\n",
       "         -1.35384426e-02, -1.31445006e-01,  1.34452403e-01,\n",
       "         -2.62375504e-01,  2.34456301e-01,  2.83662528e-02,\n",
       "         -2.09886298e-01, -3.05502918e-02,  1.57777593e-01,\n",
       "         -5.51501550e-02,  1.64380893e-01,  2.22228896e-02,\n",
       "         -5.73452771e-01,  3.29500958e-02,  5.84007129e-02,\n",
       "          3.07145059e-01,  6.83346331e-01,  1.92818537e-01,\n",
       "         -1.09429322e-01, -1.18476432e-02, -3.61111253e-01,\n",
       "          1.22809976e-01, -2.48888656e-01, -2.59786546e-01,\n",
       "          9.23014954e-02,  1.79273561e-02,  6.88984916e-02,\n",
       "         -2.32863482e-02,  2.00894639e-01,  5.94646262e-04,\n",
       "          2.12793171e-01, -1.77915677e-01, -1.31947339e-01,\n",
       "          1.72272712e-01, -2.80353695e-01,  2.49938682e-01,\n",
       "         -2.25906849e-01,  1.03908628e-02, -7.40025286e-03,\n",
       "          1.81239948e-01, -6.95463344e-02, -1.64569080e-01,\n",
       "         -6.69435859e-02, -1.08501665e-01,  1.05925404e-01,\n",
       "          1.09195828e-01, -5.00342995e-02,  7.53698349e-02,\n",
       "          3.29649374e-02,  1.23288065e-01, -1.33872226e-01,\n",
       "         -1.81044251e-01, -1.62980065e-01, -1.70456633e-01,\n",
       "          2.01841623e-01,  1.55207172e-01,  1.85121208e-01,\n",
       "         -2.61182904e-01,  1.68330640e-01,  2.64986977e-03,\n",
       "          1.02200538e-01,  8.10508132e-02, -4.93478961e-02,\n",
       "         -3.78216892e-01, -2.73326308e-01, -1.51387647e-01,\n",
       "         -1.21450878e-03, -2.05862984e-01]], dtype=float32),\n",
       " array([[ 0.1887666 , -0.26210612, -0.13908762,  0.04180605,  0.34831616,\n",
       "         -0.18137118, -0.19860779,  0.28507066, -0.26526228, -0.01607917,\n",
       "          0.12393729, -0.22584313, -0.23127508, -0.16851197, -0.29581058,\n",
       "         -0.04600808,  0.25070146,  0.02206033,  0.06916054, -0.06072301,\n",
       "         -0.11878604,  0.14321646,  0.19690132, -0.23676771, -0.17861202,\n",
       "         -0.10632949,  0.12409066,  0.3406178 ,  0.14950927,  0.09566545,\n",
       "         -0.36429566,  0.06796338, -0.04332128,  0.04407374,  0.1652198 ,\n",
       "          0.00769531,  0.03417064,  0.10593835,  0.00442976,  0.16431339,\n",
       "         -0.10921237,  0.22507624,  0.01969198,  0.0160215 , -0.19539492,\n",
       "         -0.05612478, -0.13401517, -0.20950456,  0.07305866, -0.08875521,\n",
       "         -0.0892967 , -0.15412103,  0.03941587, -0.16718401,  0.19150555,\n",
       "          0.02344405,  0.0821787 , -0.04466959,  0.141769  ,  0.20091258,\n",
       "         -0.15922718,  0.12381759, -0.03261156, -0.01591453, -0.16773894,\n",
       "         -0.05831132, -0.22886081,  0.3164901 , -0.13423954,  0.18190575,\n",
       "          0.00857644,  0.03841563, -0.38974437,  0.02608731, -0.14065307,\n",
       "         -0.10861098, -0.08950004,  0.12058165, -0.06173276,  0.27671823,\n",
       "         -0.00607082, -0.3411679 ,  0.19747968, -0.3543121 ,  0.06722666,\n",
       "          0.04047184,  0.22018172,  0.42151418, -0.10392071,  0.20357768,\n",
       "          0.05175982,  0.22705719,  0.41014916, -0.03470019, -0.03497833,\n",
       "         -0.16103609,  0.34198344,  0.19580404, -0.17630471,  0.07903396,\n",
       "         -0.07609191, -0.25914386,  0.17482962, -0.09202425, -0.11914458,\n",
       "          0.10038316,  0.10535778,  0.24233231,  0.27512756, -0.02770063,\n",
       "          0.12768076, -0.23629536,  0.21086791, -0.02526155,  0.10249043,\n",
       "         -0.08953425,  0.04947279,  0.11408986, -0.19485484,  0.28886914,\n",
       "          0.27269253, -0.15037209, -0.25297275, -0.14802141,  0.21887088,\n",
       "          0.30375138,  0.08848716,  0.00981394]], dtype=float32),\n",
       " array([[ 0.1887666 , -0.26210612, -0.13908762,  0.04180605,  0.34831616,\n",
       "         -0.18137118, -0.19860779,  0.28507066, -0.26526228, -0.01607917,\n",
       "          0.12393729, -0.22584313, -0.23127508, -0.16851197, -0.29581058,\n",
       "         -0.04600808,  0.25070146,  0.02206033,  0.06916054, -0.06072301,\n",
       "         -0.11878604,  0.14321646,  0.19690132, -0.23676771, -0.17861202,\n",
       "         -0.10632949,  0.12409066,  0.3406178 ,  0.14950927,  0.09566545,\n",
       "         -0.36429566,  0.06796338, -0.04332128,  0.04407374,  0.1652198 ,\n",
       "          0.00769531,  0.03417064,  0.10593835,  0.00442976,  0.16431339,\n",
       "         -0.10921237,  0.22507624,  0.01969198,  0.0160215 , -0.19539492,\n",
       "         -0.05612478, -0.13401517, -0.20950456,  0.07305866, -0.08875521,\n",
       "         -0.0892967 , -0.15412103,  0.03941587, -0.16718401,  0.19150555,\n",
       "          0.02344405,  0.0821787 , -0.04466959,  0.141769  ,  0.20091258,\n",
       "         -0.15922718,  0.12381759, -0.03261156, -0.01591453, -0.16773894,\n",
       "         -0.05831132, -0.22886081,  0.3164901 , -0.13423954,  0.18190575,\n",
       "          0.00857644,  0.03841563, -0.38974437,  0.02608731, -0.14065307,\n",
       "         -0.10861098, -0.08950004,  0.12058165, -0.06173276,  0.27671823,\n",
       "         -0.00607082, -0.3411679 ,  0.19747968, -0.3543121 ,  0.06722666,\n",
       "          0.04047184,  0.22018172,  0.42151418, -0.10392071,  0.20357768,\n",
       "          0.05175982,  0.22705719,  0.41014916, -0.03470019, -0.03497833,\n",
       "         -0.16103609,  0.34198344,  0.19580404, -0.17630471,  0.07903396,\n",
       "         -0.07609191, -0.25914386,  0.17482962, -0.09202425, -0.11914458,\n",
       "          0.10038316,  0.10535778,  0.24233231,  0.27512756, -0.02770063,\n",
       "          0.12768076, -0.23629536,  0.21086791, -0.02526155,  0.10249043,\n",
       "         -0.08953425,  0.04947279,  0.11408986, -0.19485484,  0.28886914,\n",
       "          0.27269253, -0.15037209, -0.25297275, -0.14802141,  0.21887088,\n",
       "          0.30375138,  0.08848716,  0.00981394]], dtype=float32),\n",
       " array([[-7.58813173e-02, -1.70471162e-01,  1.42801538e-01,\n",
       "          4.30163229e-03, -8.49934369e-02, -2.07287841e-03,\n",
       "          1.66596681e-01, -2.73398787e-01, -1.91016793e-01,\n",
       "          6.14722036e-02,  1.08528055e-01, -2.04227939e-01,\n",
       "          2.08950248e-02,  4.78648208e-02,  3.42147872e-02,\n",
       "          1.55087262e-01,  7.61337765e-03,  1.60828158e-01,\n",
       "          1.22714490e-01,  1.37916192e-01, -1.59605026e-01,\n",
       "          8.76439735e-02,  1.46073222e-01,  1.59195691e-01,\n",
       "          2.23741144e-01, -1.59230664e-01, -1.75430164e-01,\n",
       "          2.53810257e-01,  8.44649449e-02, -1.10958219e-01,\n",
       "         -3.68570127e-02, -6.37841038e-03, -2.83028223e-02,\n",
       "         -7.05123767e-02, -1.52875111e-01, -1.11306801e-01,\n",
       "          2.61690957e-03,  2.07705610e-02, -5.21911047e-02,\n",
       "         -1.49811238e-01, -2.30497830e-02, -2.82098539e-03,\n",
       "          1.72418490e-01,  1.31222263e-01,  1.31423458e-01,\n",
       "          7.26999491e-02, -9.07188356e-02,  1.31778210e-01,\n",
       "          1.21087193e-01,  1.35566697e-01, -1.11307599e-01,\n",
       "         -1.67164002e-02, -2.22226623e-02,  1.24959230e-01,\n",
       "         -1.35625064e-01, -1.66438505e-01, -3.16415392e-02,\n",
       "          1.63595632e-01, -1.55787870e-01, -2.14692801e-02,\n",
       "         -7.96825513e-02,  1.44115895e-01,  9.44624841e-02,\n",
       "          6.23378679e-02,  2.33873837e-02,  8.71660467e-03,\n",
       "         -7.68161416e-02,  1.76699102e-01, -7.73783550e-02,\n",
       "          8.06669239e-03, -3.07329744e-02, -6.91174157e-03,\n",
       "          5.87295666e-02,  1.24001391e-01,  9.58102047e-02,\n",
       "         -1.49723798e-01,  2.44995616e-02,  6.39713332e-02,\n",
       "          1.40517756e-01, -1.44108087e-01,  6.45097122e-02,\n",
       "         -2.55336892e-02,  1.35060027e-01,  5.99070750e-02,\n",
       "         -1.01075567e-01,  2.77647734e-01,  1.16570126e-02,\n",
       "          1.22080192e-01, -7.33335316e-02,  2.07391139e-02,\n",
       "         -1.02015033e-01,  1.55689210e-01,  3.31670076e-01,\n",
       "          1.04505293e-01, -3.69594134e-02, -1.19701758e-01,\n",
       "         -4.24803458e-02, -1.27602428e-01,  9.84771922e-02,\n",
       "          4.79202494e-02, -1.58191547e-01, -5.70026040e-02,\n",
       "         -6.93161860e-02,  4.49307525e-04, -1.50996745e-01,\n",
       "         -1.60487682e-01,  1.70119226e-01, -1.72029843e-03,\n",
       "          2.65428036e-01,  9.77619812e-02, -7.98155516e-02,\n",
       "         -3.94281782e-02,  1.31978661e-01, -7.82596171e-02,\n",
       "         -1.89061195e-01,  6.30262271e-02, -5.35482280e-02,\n",
       "          1.58200473e-01, -4.38779965e-02,  7.83679038e-02,\n",
       "         -8.16622302e-02, -1.05294630e-01, -1.83309704e-01,\n",
       "         -4.00747545e-03,  6.32964000e-02, -1.00012295e-01,\n",
       "         -2.32136998e-04, -9.51551795e-02]], dtype=float32),\n",
       " array([[-0.07275191,  0.02378193,  0.0867899 , -0.19778307,  0.0606869 ,\n",
       "         -0.13176788, -0.00200765, -0.18466577, -0.00810413, -0.1101458 ,\n",
       "         -0.01120322, -0.12874413,  0.15329747, -0.151385  , -0.17823043,\n",
       "         -0.03907532,  0.03985414,  0.12887853,  0.13171493,  0.17887814,\n",
       "          0.03945526,  0.00149782, -0.0603374 , -0.1172009 ,  0.12284993,\n",
       "         -0.17282398,  0.05146456,  0.26172152, -0.13090919, -0.07260103,\n",
       "          0.10574902,  0.0937095 ,  0.07168078, -0.01380187, -0.06379616,\n",
       "          0.03516962, -0.01098499,  0.04410039, -0.12642239, -0.00149143,\n",
       "          0.01438981, -0.02776314,  0.15730633,  0.12142006,  0.076035  ,\n",
       "         -0.18508525, -0.04859084,  0.07291868, -0.07918579,  0.15503131,\n",
       "         -0.12089657,  0.02685558,  0.15041135,  0.02023126,  0.16722532,\n",
       "         -0.04343187,  0.0755003 , -0.13727069, -0.11966516, -0.11988725,\n",
       "          0.02285858,  0.0243728 , -0.1671469 ,  0.07430752,  0.01496097,\n",
       "          0.14039186, -0.09474258,  0.0577786 , -0.08280311, -0.2050112 ,\n",
       "          0.16666485,  0.10228673, -0.102969  , -0.20336345,  0.05153219,\n",
       "         -0.01980513, -0.05011047,  0.09263088,  0.04047358,  0.04411166,\n",
       "         -0.11425076,  0.03902171,  0.00363939, -0.17712541,  0.09817019,\n",
       "          0.1440447 ,  0.0007309 ,  0.01941338,  0.02441053, -0.06786733,\n",
       "         -0.00322177,  0.1453984 ,  0.22685075,  0.1808466 ,  0.00860577,\n",
       "         -0.05157381,  0.16249573, -0.03555574, -0.02442525,  0.14783823,\n",
       "         -0.13855651, -0.18389013, -0.02561035,  0.0199203 ,  0.11939902,\n",
       "          0.17057538,  0.1379263 ,  0.18416488,  0.08139688, -0.0554131 ,\n",
       "          0.09893472, -0.09988142, -0.10308263,  0.18360175,  0.0692981 ,\n",
       "         -0.00304621,  0.14830138,  0.09848153,  0.15075369,  0.20497562,\n",
       "          0.0603326 ,  0.03019649,  0.06181135, -0.12546414, -0.09250371,\n",
       "          0.11596479, -0.04715652,  0.1821465 ]], dtype=float32),\n",
       " array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " array([[ 0.00546503,  0.01954422,  0.08256027, ...,  0.02818499,\n",
       "          0.05255021, -0.01680645],\n",
       "        [-0.04696605, -0.06621581,  0.0601731 , ..., -0.0835178 ,\n",
       "         -0.0634388 , -0.08045986],\n",
       "        [-0.06043672, -0.07300101, -0.06770831, ..., -0.05581969,\n",
       "         -0.04356233,  0.07235858],\n",
       "        ...,\n",
       "        [ 0.03655443, -0.01138042, -0.05540288, ..., -0.04915269,\n",
       "         -0.0432975 ,  0.05594762],\n",
       "        [-0.06446978, -0.0831775 , -0.04015609, ...,  0.11725755,\n",
       "          0.00132428,  0.00451843],\n",
       "        [-0.00604601,  0.0396715 , -0.06300539, ...,  0.02212482,\n",
       "         -0.01112557,  0.02121202]], dtype=float32),\n",
       " array([[ 0.11470351,  0.02784922,  0.03018722, ..., -0.05989008,\n",
       "          0.13167517,  0.06065523],\n",
       "        [ 0.04217404, -0.00571388,  0.1378965 , ...,  0.00877859,\n",
       "         -0.08638843, -0.10144822],\n",
       "        [ 0.02440787,  0.03035395,  0.0447209 , ..., -0.16203862,\n",
       "          0.07769634, -0.07771223],\n",
       "        ...,\n",
       "        [-0.05360438, -0.07968942,  0.0985397 , ..., -0.10634626,\n",
       "         -0.01707984,  0.00152306],\n",
       "        [ 0.10959689,  0.0285446 , -0.04792498, ...,  0.01883594,\n",
       "          0.0959926 ,  0.02160209],\n",
       "        [-0.0820566 , -0.06327011, -0.03374658, ...,  0.10938581,\n",
       "          0.00516067,  0.02198885]], dtype=float32),\n",
       " array([[ 0.02667446, -0.14339018, -0.0658531 ],\n",
       "        [ 0.09478016,  0.07503221,  0.09938813],\n",
       "        [ 0.07843959, -0.042329  ,  0.0941717 ],\n",
       "        [-0.13385874,  0.05003714,  0.12417511],\n",
       "        [ 0.05505826,  0.21935193,  0.09959395],\n",
       "        [ 0.22008948,  0.15347303,  0.1849816 ],\n",
       "        [ 0.13271669, -0.10945351, -0.12995254],\n",
       "        [ 0.00881053,  0.09131987, -0.1609052 ],\n",
       "        [ 0.23330338,  0.14045438,  0.21330728],\n",
       "        [-0.08585636, -0.13791808,  0.18619353],\n",
       "        [ 0.00084815, -0.1993779 , -0.20668498],\n",
       "        [-0.20187593,  0.08731906,  0.13133974],\n",
       "        [ 0.02092847, -0.08145431,  0.11375992],\n",
       "        [-0.16555916,  0.0496708 , -0.1147357 ],\n",
       "        [-0.01876367,  0.06887168,  0.19003135],\n",
       "        [-0.02186184,  0.03684615, -0.20185201],\n",
       "        [-0.26424402, -0.20758799, -0.01651471],\n",
       "        [ 0.17837317, -0.15027332, -0.06280748],\n",
       "        [-0.09899105, -0.09638388, -0.12500428],\n",
       "        [-0.20985939, -0.00869226, -0.01249089],\n",
       "        [ 0.12597127,  0.16045536,  0.02356563],\n",
       "        [-0.07159994, -0.16210867, -0.24095495],\n",
       "        [-0.0889954 ,  0.11228861,  0.0400408 ],\n",
       "        [ 0.1960701 ,  0.05016597, -0.07164631],\n",
       "        [-0.10405607,  0.17085819,  0.24552694],\n",
       "        [ 0.11498722,  0.15576912,  0.04655733],\n",
       "        [ 0.10181373, -0.12287817, -0.08100615],\n",
       "        [ 0.08317953,  0.16206804,  0.21707252],\n",
       "        [ 0.00479094, -0.15812299,  0.17502388],\n",
       "        [-0.08531558, -0.05309659,  0.12722966],\n",
       "        [ 0.05440991, -0.05317163, -0.19246435],\n",
       "        [ 0.01696669, -0.08583584, -0.1914789 ]], dtype=float32)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[g['weights'] for g in grouped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trained_weights_video2_reg0', grouped, allow_pickle=True, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path_to_video, down_sample_rate):\n",
    "    vid = imageio.get_reader(path_to_video,  'ffmpeg')\n",
    "    h, w, c = vid.get_data(0).shape\n",
    "    dframes = int(np.ceil(vid.count_frames()/down_sample_rate))\n",
    "    pix = np.zeros((h*w*dframes,c))\n",
    "    for idx, frame in enumerate(np.arange(0, vid.count_frames(), step = down_sample_rate)):#, step = down_sample_rate)):\n",
    "        pix[h*w*idx:h*w*(idx+1),:] = vid.get_data(frame).reshape(h*w, c)\n",
    "    #vid = skvideo.io.vread(path_to_video) \n",
    "    return pix, h, w, vid.count_frames()\n",
    "    \n",
    "def load_bias(path_to_bias, h, w):\n",
    "    biasimage = io.imread(path_to_bias)\n",
    "    #h, w, c = 480, 640, 3\n",
    "    #biasimage = rescale(biasimage, 0.25, anti_aliasing=False)\n",
    "    biasimage = resize(biasimage, (h, w), anti_aliasing=True)\n",
    "    \n",
    "    return biasimage.reshape(h*w, 3)\n",
    "    \n",
    "def load_eeg(path_to_eeg, down_sample_rate):\n",
    "    eegdata = pd.read_csv(path_to_eeg, sep = ' ', header=None)\n",
    "    eegdata = eegdata.iloc[::down_sample_rate, :]\n",
    "    return eegdata\n",
    "    \n",
    "def generate_gradient(h, w, axisrange=1):\n",
    "    return(np.meshgrid(np.linspace(-axisrange, axisrange, num=h),np.linspace(-axisrange, axisrange, num=w))[0])\n",
    "    \n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_video, path_to_eeg, path_to_bias, down_sample_rate=10, transform = None):\n",
    "        \n",
    "        self.video, h, w, num_video_frames = load_video(path_to_video, down_sample_rate)\n",
    "        # h, w, color_channels, num_video_frames = self.video.shape\n",
    "        # self.video = np.reshape(self.video, h*w*num_video_frames, time_axis)\n",
    "        self.eeg = load_eeg(path_to_eeg, down_sample_rate)\n",
    "        num_eeg_frames, _ = self.eeg.shape\n",
    "        if (num_eeg_frames == num_video_frames):\n",
    "             num_ref_frames = num_eeg_frames \n",
    "        else:\n",
    "            num_ref_frames = np.min([num_eeg_frames, num_video_frames])  \n",
    "        self.eeg = self.eeg.loc[self.eeg.index.repeat(h*w)].to_numpy() \n",
    "        xx, yy = generate_gradient(h, w), generate_gradient(w, h, axisrange=w/h).transpose()\n",
    "        zz = np.sqrt(xx**2 + yy**2)\n",
    "        gframes = np.repeat(np.concatenate((xx[:,:,np.newaxis], yy[:,:,np.newaxis], zz[:,:,np.newaxis]), axis=2)\n",
    "                            .reshape(h*w, 3), \n",
    "                            num_ref_frames, axis=0) # int(num_ref_frames/down_sample_rate)\n",
    "        eframes = self.eeg[:num_ref_frames*h*w, :]\n",
    "        vframes = self.video[:num_ref_frames*h*w, :]\n",
    "        gframes = gframes[:num_ref_frames*h*w:down_sample_rate, :]\n",
    "        print(eframes.shape, vframes.shape, gframes.shape)\n",
    "        self.data = np.concatenate((gframes, vframes, eframes),axis=1)\n",
    "        self.bias = np.repeat(load_bias(path_to_bias, h, w), int(np.ceil(num_ref_frames/down_sample_rate)),axis=0)\n",
    "        self.transform = transform\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index,:]\n",
    "        label = self.bias[index,:]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
